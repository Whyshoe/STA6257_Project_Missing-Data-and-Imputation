    ---
    title: "Missing Data and Imputations- Data Science Capstone"
    author: "Vyshnavi"
    date: '`r Sys.Date()`'
    format:
      html:
        code-fold: true
    course: STA 6257 - Advanced Statistical Modeling
    bibliography: references.bib # file contains bibtex for references
    #always_allow_html: true # this allows to get PDF with HTML features
    self-contained: true
    execute: 
      warning: false
      message: false
    editor: 
      markdown: 
        wrap: 72
    ---


### Introduction ###

***Assignment: 4 ***

***Dealing with missing data in a multi-question depression scale: a comparison of imputation methods***
###FM Shrive, H Stuart, H Quan, WA Ghali - BMC medical research …, 2006 – Springer###

***Goal of the Paper:***
The goal of this paper is to address the challenge of missing data in research projects, particularly in studies using self-report scales, with a focus on the Zung Self-reported Depression scale (SDS). The paper aims to compare and evaluate six different imputation techniques for handling missing data in the SDS. The overarching objective is to provide insights into the effectiveness of various imputation methods in different missing data scenarios, considering factors such as validity, ease of interpretability, and statistical expertise.

***Methods:***
The study involves 1580 participants from a surgical outcomes study who completed the SDS, a 20-question scale assessing depressive symptoms. Missing values are simulated using three scenarios: missing completely at random (MCAR), missing at random (MAR), and missing not at random (MNAR). Six imputation techniques are compared: multiple imputation, single regression, individual mean, overall mean, participant's preceding response, and random selection. The evaluation involves comparing imputed mean SDS scores, standard deviations, Spearman correlation coefficient, percent misclassified, and the Kappa statistic against population statistics.

***Results:***
- Imputation Methods Evaluation (10% Missing): Multiple imputation (MI) stands out with the highest Kappa statistic (0.89), indicating 'near perfect' agreement. Single regression and individual mean imputation also yield favorable results. Random selection performs less well.
- Effect of Increasing Missing Data (20%, 30%): MI maintains high Kappa statistics even as the percent of missing information increases. Individual mean and single regression show substantial agreement but with a slight decrease. Other methods exhibit declines in performance.
- Unbalanced Missing Data Scenarios: MI generally performs well, but individual mean outperforms MI in some unbalanced scenarios.

***Conclusion:***
The paper concludes that multiple imputation is the most accurate method for handling missing data in most scenarios assessed for the SDS. Individual mean imputation, a simpler approach, is also deemed appropriate, demonstrating comparable accuracy and interpretability. The authors emphasize the importance of considering methodological assessments when confronted with missing data, suggesting a balance between validity, interpretability, and the expertise of the research team in selecting the optimal imputation method.


***Dealing with missing data in a multi-question depression scale: a comparison of imputation methods***
###FM Shrive, H Stuart, H Quan, WA Ghali, BMC medical research methodology, 2006•Springer***


***Goal of the Paper:***
The goal of the paper is to address the crucial issue of missing data in machine learning and data mining. The authors emphasize the significance of data quality in these fields and highlight the challenges posed by missing values, which can lead to biased results in knowledge discovery. The paper aims to introduce and implement an imputation approach using the IBK (k-Nearest Neighbors) classification algorithm to handle missing values in a dataset.

***Methods:***
1. Introduction: The paper begins by discussing the importance of data quality in machine learning and data mining, with a focus on handling missing values.
2. Background: The background section provides an overview of data mining, preprocessing techniques, missing values, and imputation methods, particularly focusing on the K-Nearest Neighbors (KNN) algorithm for imputation.
3. Related Work: The authors review related work on missing data imputation, discussing different approaches and their applications.
4. Experimental Procedures: The paper outlines the experimental procedures, including the dataset used (from Juba Insurance & Reinsurance Company), the introduction of artificial missing values, and the proposed imputation method using the IBK algorithm.

***Results:***
The authors implement the proposed IBK imputation algorithm using the k-NN approach to handle missing values in the dataset. They demonstrate the process step-by-step, including data preprocessing, normalization, imputation, and analysis of the imputed dataset. The results are presented using visualizations, summary statistics, and a comparison between the original and imputed datasets.

***Conclusion:***
The paper concludes by summarizing the achieved results and the effectiveness of the proposed imputation approach. The authors emphasize the importance of data accuracy in knowledge discovery and suggest that the presented method can contribute to improving the quality of datasets used in machine learning and data mining. They also discuss potential future directions, such as exploring larger datasets and investigating missingness mechanisms automatically.
In essence, the paper addresses the challenge of missing data in the context of machine learning and data mining, proposing a specific imputation method using the IBK classification algorithm.
















### What is "method"?

This is an introduction to Kernel regression, which is a non-parametric
estimator that estimates the conditional expectation of two variables
which is random. The goal of a kernel regression is to discover the
non-linear relationship between two random variables. To discover the
non-linear relationship, kernel estimator or kernel smoothing is the
main method to estimate the curve for non-parametric statistics. In
kernel estimator, weight function is known as kernel function
[@efr2008]. Cite this paper [@bro2014principal]. The GEE [@wang2014].

This is my work and I want to add more work...

### Related work

This section is going to cover the literature review...

## Methods

The common non-parametric regression model is
$Y_i = m(X_i) + \varepsilon_i$, where $Y_i$ can be defined as the sum of
the regression function value $m(x)$ for $X_i$. Here $m(x)$ is unknown
and $\varepsilon_i$ some errors. With the help of this definition, we
can create the estimation for local averaging i.e. $m(x)$ can be
estimated with the product of $Y_i$ average and $X_i$ is near to $x$. In
other words, this means that we are discovering the line through the
data points with the help of surrounding data points. The estimation
formula is printed below [@R-base]:

$$
M_n(x) = \sum_{i=1}^{n} W_n (X_i) Y_i  \tag{1}
$$ $W_n(x)$ is the sum of weights that belongs to all real numbers.
Weights are positive numbers and small if $X_i$ is far from $x$.

Another equation:

$$
y_i = \beta_0 + \beta_1 X_1 +\varepsilon_i
$$

## Analysis and Results

### Data and Visualization

A study was conducted to determine how...

```{r, warning=FALSE, echo=T, message=FALSE}
# loading packages 
library(tidyverse)
library(knitr)
library(ggthemes)
library(ggrepel)
library(dslabs)
```

```{r, warning=FALSE, echo=TRUE}
# Load Data
kable(head(murders))

ggplot1 = murders %>% ggplot(mapping = aes(x=population/10^6, y=total)) 

  ggplot1 + geom_point(aes(col=region), size = 4) +
  geom_text_repel(aes(label=abb)) +
  scale_x_log10() +
  scale_y_log10() +
  geom_smooth(formula = "y~x", method=lm,se = F)+
  xlab("Populations in millions (log10 scale)") + 
  ylab("Total number of murders (log10 scale)") +
  ggtitle("US Gun Murders in 2010") +
  scale_color_discrete(name = "Region")+
      theme_bw()
  

```

### Statistical Modeling

```{r}

```

### Conclusion

## References
